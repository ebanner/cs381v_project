\section{Methods}

We seek to extend the method of \cite{zhao2011large} to deep convolutional
neural networks.  We show that by using soft labels, our model can achieve
better classification performance when compared to a model trained with 1-hot
vectors.

% In addition, we also see if soft labels can
% help reduce the required number of training examples.

%Based on the work of
%\cite{hinton2015distilling}, we will see if we can reduce the architectural
%complexity of our network (thus reducing training and evaluation time).

We compare different schemes for obtaining soft labels for ImageNet categories.
In doing so, we explore semantic similarity measures derived from word
embeddings trained on large amounts of text, WordNet, as well as a visual
similarity metric based on GIST.


\subsection{Soft Labels}
\label{sec:soft_labels}

\begin{table}[!tb]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
      & Sax & Limo & Church & Yurt & Obelisk \\
    \hline
      \textbf{1-hot} & 1 & 0 & 0 & 0  & 0 \\
    \hline
      \textbf{Soft} & 1 & 0.5 & 0.14 & 0.43 & 0.43 \\
    \hline
  \end{tabular}
  \caption{
    A \emph{1-hot label} vector vs. a \emph{soft label} vector for
    ``saxophone'' among 5 classes.
  }
  \label{tbl:soft_labels}
\end{table}

Traditionally when training a classifier, the loss function is computed by
comparing the prediction values against a 1-hot label vector. A 1-hot label
means that the correct (ground truth) category is given a value of 1 and all
other categories are given a value of 0. When the classifier makes an error, it
is not given any ``partial credit'' based on the semantic relevance of its
mistake.

On the contrary, soft labels provide a real-valued distribution over the object
categories. Thus, the classifier is not penalized as heavily for making a
semantically relevant error (e.g. misclassifying a ``donkey'' as a ``mule'') as
it would for a completely incorrect prediction (e.g. misclassifying a ``truck''
as a ``cat''). The use of soft labels also encourages assigning a small amount
of probability mass to semantically-related classes. An example is shown in
Table \ref{tbl:soft_labels}.

A soft labeling scheme can be visualized by an \emph{affinity matrix}. Figure
\ref{fig:aff-5_1} shows a five class example. An affinity matrix $A$ is defined
as follows: element $A_{ij}$ is the similarity (i.e. \emph{affinity}) between
class $i$ and class $j$. Affinity matrices are typically symmetric (i.e. $A_{ij}
= A_{ji}$ for all $i, j$), although we violate this constraint in the word2vec
similarity scheme (for an example see Section \ref{word2vec}). We keep the
diagonal of $A$ fixed to be 1 as a class is maximally similar to itself.

\begin{figure}[!tb]
%\begin{figure}[t]
  \centering
  \includegraphics[width=0.4\textwidth]{figs/aff-5_1.png}
  \caption{
      Affinity matrix for five ImageNet classes computed from word2vec word
      similarity. Note that \emph{church}, \emph{yurt}, and
      \emph{obelisk} have high similarity, due to their strutural nature, while
      \emph{limousine} and \emph{sax} have low similarity to most other classes.
  }
  \label{fig:aff-5_1}
\end{figure}

\subsection{Linguistic Semantic Similarity}


\subsubsection{WordNet}

WordNet is a large database of English words grouped into synonym sets called
sysnets \cite{miller1995wordnet}.
The sysnets are related hierarchically with semantic and conceptual links, such
as hypernyms (``is a'') and meronyms (``part of'') relations.

Using the distance metric in Equation \ref{eq:wordnet_dist}, we follow the work
of \cite{fergus2010semantic} and \cite{zhao2011large} for defining soft labels
for a deep CNN classifier. We experimentally test a wide variety of
hyperparameters and values for $\kappa$ (defined in Section
\ref{sec:related_works}, which measures how diffuse or sparse the similarity
distribution is. We find that values between 3 and 7 work best.

We use three different similarity metrics between hypernyms in WordNet.
First, we use Equation \ref{eq:wordnet_dist} as defined in \cite{zhao2011large}.
We also use the WUP distance metric (as defined in
\cite{budanitsky2006evaluating}) and the PATH distance metric:
\begin{equation}
\label{eq:path_dist}
\mathrm{PATH}(\mathrm{label}_i, \mathrm{label}_j) =
  \frac{1}{\mathrm{path\_len}(\mathrm{label}_i, \mathrm{label}_j)}
\end{equation}
In general, we found that affinity matrices constructed with equations
\ref{eq:wordnet_dist} and \ref{eq:path_dist} work best.


\subsubsection{Word Embeddings} \label{word2vec}

A word embedding maps each word in a lexicon to a continuous
real-valued vector. The nature of this mapping is often guided by the
distributional hypothesis introduced in
\cite{harris1954distributional}, which posits that a word can be
understood by the context it appears in. Assuming this hypothesis, a
desirable mapping is one that maps words which appear in similar
contexts to nearby locations in the embedding space.

Popular methods for computing word embeddings such as word2vec
\cite{mikolov2013distributed} and GloVe \cite{pennington2014glove} have been
shown to capture such properties.  However, \cite{levy2014dependency} shows that
when context is varied, the notion of similarity between words also changes. For
example, when context is taken to be neighboring words, words close in the
embedding space tend to be \emph{topically} similar. Conversely, when context is
defined via linguistic dependencies, nearby words tend to be \emph{semantically}
similar. We examine the effect word context has on performance.

We construct an affinity matrix $A$ using word vectors as follows: for
the off diagonals, we set $A_{ij}$ to be the cosine similarity between the word
vectors corresponding to label $i$ and label $j$. We perform the additional step
of applying the scaling procedure and normalization procedure to $A$: for each
row of $A$, we apply the scaling procedure of \cite{zhao2011large} to all the
elements with the main diagonal element removed and normalize these values to
sum to $s$, where $s$ is a hyperparameter. We found a value of $s = 0.5$ to work
well.  We performed this additional scaling and normalization procedure because
simply applying the scaling procedure of \cite{zhao2011large} to the each row
(i.e.  including the main diagonal element) resulted in relatively equal
probability mass for each off-diagonal entry of $A$. Additionally, each of these
off-diagonal probability masses were near-zero, hence normalizing them to sum up
to 0.5 prevents perpetually negative gradient signals for off-diagonal classes.

A side effect of this scaling and normalization procedure to each row of the
affinity matrix is that $A$ is no longer symmetric. For the most part we observe
$A_{ij} \approx A_{ji}$, as can be seen in Figure \ref{fig:aff-5_1} The most
drastic asymmetries occur when class $i$ is uniformly similar to all classes and
class $j$ has high similarity with one or two classes. For example, \emph{sax}
has relatively uniform similarity to all other classes, while \emph{yurt} has
high similarity for \emph{church} and \emph{obelisk} and near-zero similarity to
to \emph{sax}.

\subsection{Visual Similarity}

A main focus of our work is to exploit information embedded in natural language
to aid in visual classification.  However, it has been shown that linguistic
semantics do not always work well in modeling visual similarity
\cite{li2010building}. As an example given in \cite{li2010building}, WordNet
does not capture the correlation between ``tower'' and ``business district''. To
address this issue, we explore soft labeling schemes based on visual similarity
and compare the performance to that of the linguistic schemes.

% By better understanding the relationship between linguistic similarity
% and the visual representations learned by a CNN, we hope to explore improved
% ways of defining and weighting soft labels.  We use several existing
% methods to compute the similarities between different image categories.

We compute the average GIST descriptor \cite{oliva2001modeling} for each
category across all images in the training data set and construct an affinity
matrix in a similar manner as in Section \ref{word2vec}. Since GIST is a global
descriptor, we expect it to capture more of the background contextual
information rather than similarities between the objects themselves.


\subsection{Random Soft Labels}

As a ``sanity check'' that our method is actually working, we also train the
CNN on a randomly-generated soft label distribution. For the affinity matrix
$A$, each non-diagonal entry $A_{ij}, i \neq j$ is a randomly-generated value
between 0 and 0.25. The diagonal (identity) values are fixed at 1 as in the
other soft label schemes. We generate several random affinity matrices in this
manner and take the average classification score for each data set.

%To better represent the categories directly, we can also experiment with
%visual word clusters mined from images of different object categories.
%We expect that clustering SIFT descriptors \cite{lowe1999object} should work
%reasonably well.
%The similarity metric can be defined as the distance between the cluster
%centroids into which the images of different categories fall.
%We can also explore using visual vocabulary trees, introduced by
%\cite{nister2006scalable}.
%
%We can investigate many other methods. The most notable and relevant to our
%work is \cite{li2010building}, who have successfully used their visual
%similarity approach for improving hierarchical image classification.
%We can also explore other works in unsupervised clustering, including most
%recent advancements using deep neural networks in unsupervised visual
%clustering
%(Jianwei Yang, Devi Parikh, and Dhruv Batra, 2016 - not yet published).

% Possibly the trained experts thing?
