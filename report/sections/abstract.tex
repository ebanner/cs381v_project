\begin{abstract}
  {\it

    Convolutional neural networks achieve near human-level performance
    on image classification on many important datasets. Though
    performance is impressive, these models are data hungry and
    require on the order of thousands of examples per-class. This
    property is partly due to the fact that supervision comes in the
    form of impoverished ``one-hot'' learning signals corresponding to
    singular object classes, conveying no information for how classes
    are related.

    Inspired by work in semantic label sharing, model compression, and
    the availability of external sources of linguistic knowledge, we
    aim to enrich the learning signal for image classification from a
    variety of external sources. We show that training with
    soft-labels allow us to train models with fewer parameters and
    data, while achieving performance comparable to larger models
    trained on more examples in the traditional way. We evaluate with
    standard accuracy and semantic-relatedness measures and
    demonstrate that our model make more semantically-sensible errors.

}
\end{abstract}
