\section{Conclusion}

We found that training with soft labels boosts overall classification accuracy,
especially in regimes where labeled data is sparse. We were able to do this
\emph{cheaply} with off the shelf linguistic resources and simple image
features. We also confirmed our hypothesis that the neural networks trained on
soft labels generally make more semantically reasonable errors than those trained
the traditional way.


\subsection{Future Work}

We believe that these results show promise for using soft labels, but we have
only begun to scratch the surface. First, we would like to run many more
experiments with different hyperparameters and affinity matrix normalizations,
and take the average performance scores of many repeated iterations of the same
experiment. We can also explore different corpora for learning word embeddings
and many other WordNet distance metrics, and compare how each does on different
data sets. Additionally, we observed that models trained on soft labels achieved
their maximum validation accuracy in fewer epochs than those trained on 1-hot
labels and would like to run more experiments to confirm this finding.  

Next, given that we saw more improvement on the C10 class set than on C5, we
would like to expand the size and scope of our training data. Ultimately, we
would like to train on the entirety of ImageNet, and use data augmentation
techniques to come closer to current state-of-the-art performance. Another
natural extension is to train a wider variety of deep neural network
architectures, including simpler models (with fewer parameters) to test our
hypothesis that we can train simpler models that achieve strong performance, as
shown in \cite{hinton2015distilling}.

We are also interested in generating soft labels at the \emph{image} level. In
this setting, class-level semantic similarities could be used as a weak prior,
which could be refined by image-level visual features. One could even
potentially use a weak classifier to further refine these soft labels. Such an
approach would be one step in the direction of uniting the distillation work in
\cite{hinton2015distilling} and work in more traditional semantic label sharing
settings.

Finally, we would like to see how well our method extends to pre-trained models
(fine-tuning) and whether we can get any reasonable performance in zero-shot
learning. Zero-shot learning could potentially be achieved by training the CNN
on images of classes that are semantically similar to a class which we have no
training examples for. By emphasizing that certain features contribute to the
unseen object category through soft labels, we believe that some information
necessary to identify the unknown class would be learned by the model.
