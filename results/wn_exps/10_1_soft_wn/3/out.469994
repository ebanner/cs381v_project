Experiment parameters:
   exp_group = 10_1_soft_wn, exp_id = 3
   Data file (image data): pickle_jar/10_1-1260.p
   nb_epoch = 50, batch_size = 32, model_name = "vgg16"
   Using affinity matrix: data_files/10_1/aff_wordnet_zhao
      soft_label_decay_factor = 5.0
   Validating every 1 epochs.
   Weights are NOT being saved.
   Weights are NOT being loaded.
Loading pickled data...
Done in 2.51 minutes.
Loading affinity matrix for soft labels from text file...
[[ 1.          0.5         0.5         0.14285715  0.33333334  0.14285715
   0.42857143  0.42857143  0.27777779  0.35714287]
 [ 0.5         1.          0.5         0.14285715  0.33333334  0.14285715
   0.42857143  0.42857143  0.27777779  0.35714287]
 [ 0.5         0.5         1.          0.14285715  0.33333334  0.25
   0.54545456  0.54545456  0.27777779  0.45454547]
 [ 0.14285715  0.14285715  0.14285715  1.          0.13333334  0.21428572
   0.14285715  0.14285715  0.11111111  0.14285715]
 [ 0.33333334  0.33333334  0.33333334  0.13333334  1.          0.13333334
   0.33333334  0.33333334  0.72222221  0.46666667]
 [ 0.14285715  0.14285715  0.25        0.21428572  0.13333334  1.          0.2
   0.22222222  0.11111111  0.25      ]
 [ 0.42857143  0.42857143  0.54545456  0.14285715  0.33333334  0.2         1.
   0.69999999  0.27777779  0.45454547]
 [ 0.42857143  0.42857143  0.54545456  0.14285715  0.33333334  0.22222222
   0.69999999  1.          0.27777779  0.45454547]
 [ 0.27777779  0.27777779  0.27777779  0.11111111  0.72222221  0.11111111
   0.27777779  0.27777779  1.          0.3888889 ]
 [ 0.35714287  0.35714287  0.45454547  0.14285715  0.46666667  0.25
   0.45454547  0.45454547  0.3888889   1.        ]]
Re-scaled soft labels.
[[ 1.          0.082085    0.082085    0.01376379  0.03567401  0.01376379
   0.05743263  0.05743263  0.0270218   0.04018403]
 [ 0.082085    1.          0.082085    0.01376379  0.03567401  0.01376379
   0.05743263  0.05743263  0.0270218   0.04018403]
 [ 0.082085    0.082085    1.          0.01376379  0.03567401  0.02351775
   0.10303081  0.10303081  0.0270218   0.06539742]
 [ 0.01376379  0.01376379  0.01376379  1.          0.01312373  0.01967176
   0.01376379  0.01376379  0.01174363  0.01376379]
 [ 0.03567401  0.03567401  0.03567401  0.01312373  1.          0.01312373
   0.03567401  0.03567401  0.24935219  0.06948346]
 [ 0.01376379  0.01376379  0.02351775  0.01967176  0.01312373  1.
   0.01831564  0.02046808  0.01174363  0.02351775]
 [ 0.05743263  0.05743263  0.10303081  0.01376379  0.03567401  0.01831564
   1.          0.22313017  0.0270218   0.06539742]
 [ 0.05743263  0.05743263  0.10303081  0.01376379  0.03567401  0.02046808
   0.22313017  1.          0.0270218   0.06539742]
 [ 0.0270218   0.0270218   0.0270218   0.01174363  0.24935219  0.01174363
   0.0270218   0.0270218   1.          0.04709655]
 [ 0.04018403  0.04018403  0.06539742  0.01376379  0.06948346  0.02351775
   0.06539742  0.06539742  0.04709655  1.        ]]
Building model...
Building "vgg16" model.
--------------------------------------------------------------------------------
Initial input shape: (None, 3, 224, 224)
--------------------------------------------------------------------------------
Layer (name)                  Output Shape                  Param #             
--------------------------------------------------------------------------------
ZeroPadding2D (zeropadding2d) (None, 3, 226, 226)           0                   
Convolution2D (convolution2d) (None, 64, 224, 224)          1792                
ZeroPadding2D (zeropadding2d) (None, 64, 226, 226)          0                   
Convolution2D (convolution2d) (None, 64, 224, 224)          36928               
MaxPooling2D (maxpooling2d)   (None, 64, 112, 112)          0                   
ZeroPadding2D (zeropadding2d) (None, 64, 114, 114)          0                   
Convolution2D (convolution2d) (None, 128, 112, 112)         73856               
ZeroPadding2D (zeropadding2d) (None, 128, 114, 114)         0                   
Convolution2D (convolution2d) (None, 128, 112, 112)         147584              
MaxPooling2D (maxpooling2d)   (None, 128, 56, 56)           0                   
ZeroPadding2D (zeropadding2d) (None, 128, 58, 58)           0                   
Convolution2D (convolution2d) (None, 256, 56, 56)           295168              
ZeroPadding2D (zeropadding2d) (None, 256, 58, 58)           0                   
Convolution2D (convolution2d) (None, 256, 56, 56)           590080              
ZeroPadding2D (zeropadding2d) (None, 256, 58, 58)           0                   
Convolution2D (convolution2d) (None, 256, 56, 56)           590080              
MaxPooling2D (maxpooling2d)   (None, 256, 28, 28)           0                   
ZeroPadding2D (zeropadding2d) (None, 256, 30, 30)           0                   
Convolution2D (convolution2d) (None, 512, 28, 28)           1180160             
ZeroPadding2D (zeropadding2d) (None, 512, 30, 30)           0                   
Convolution2D (convolution2d) (None, 512, 28, 28)           2359808             
ZeroPadding2D (zeropadding2d) (None, 512, 30, 30)           0                   
Convolution2D (convolution2d) (None, 512, 28, 28)           2359808             
MaxPooling2D (maxpooling2d)   (None, 512, 14, 14)           0                   
ZeroPadding2D (zeropadding2d) (None, 512, 16, 16)           0                   
Convolution2D (convolution2d) (None, 512, 14, 14)           2359808             
ZeroPadding2D (zeropadding2d) (None, 512, 16, 16)           0                   
Convolution2D (convolution2d) (None, 512, 14, 14)           2359808             
ZeroPadding2D (zeropadding2d) (None, 512, 16, 16)           0                   
Convolution2D (convolution2d) (None, 512, 14, 14)           2359808             
MaxPooling2D (maxpooling2d)   (None, 512, 7, 7)             0                   
Flatten (flatten)             (None, 25088)                 0                   
Dense (dense)                 (None, 4096)                  102764544           
Dropout (dropout)             (None, 4096)                  0                   
Dense (dense)                 (None, 4096)                  16781312            
Dropout (dropout)             (None, 4096)                  0                   
Dense (dense)                 (None, 10)                    40970               
--------------------------------------------------------------------------------
Total params: 134301514
--------------------------------------------------------------------------------
Done in 26.26 seconds.
Training model...
Epoch 1/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.24255283356
acc: 0.21
1040s - loss: 3.2647
Epoch 2/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.23124862671
acc: 0.265
1041s - loss: 3.2526
Epoch 3/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.22372398376
acc: 0.265
1041s - loss: 3.2453
Epoch 4/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.21753430367
acc: 0.26
1041s - loss: 3.2434
Epoch 5/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.21162516594
acc: 0.27
1040s - loss: 3.2360
Epoch 6/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.20682010651
acc: 0.28
1041s - loss: 3.2351
Epoch 7/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.20258160591
acc: 0.28
1041s - loss: 3.2320
Epoch 8/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.19868124962
acc: 0.285
1041s - loss: 3.2272
Epoch 9/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.19501075745
acc: 0.285
1041s - loss: 3.2263
Epoch 10/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.19165805817
acc: 0.29
1041s - loss: 3.2231
Epoch 11/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.18854767799
acc: 0.295
1041s - loss: 3.2234
Epoch 12/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.18568579674
acc: 0.3
1041s - loss: 3.2224
Epoch 13/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.18288816452
acc: 0.3
1041s - loss: 3.2193
Epoch 14/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.18021028519
acc: 0.3
1041s - loss: 3.2172
Epoch 15/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.17759696007
acc: 0.3
1041s - loss: 3.2143
Epoch 16/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.17520736694
acc: 0.305
1041s - loss: 3.2153
Epoch 17/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.17277164459
acc: 0.305
1041s - loss: 3.2106
Epoch 18/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.17037139893
acc: 0.31
1041s - loss: 3.2087
Epoch 19/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.16819518089
acc: 0.31
1041s - loss: 3.2098
Epoch 20/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.16616864204
acc: 0.305
1041s - loss: 3.2089
Epoch 21/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.16412548065
acc: 0.31
1041s - loss: 3.2076
Epoch 22/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.16217036247
acc: 0.305
1041s - loss: 3.2044
Epoch 23/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.16020001411
acc: 0.305
1041s - loss: 3.2028
Epoch 24/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.15837962151
acc: 0.305
1041s - loss: 3.2038
Epoch 25/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.15643875122
acc: 0.3
1041s - loss: 3.2005
Epoch 26/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.1546749115
acc: 0.3
1041s - loss: 3.2008
Epoch 27/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.1529910183
acc: 0.305
1041s - loss: 3.1995
Epoch 28/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.15132127762
acc: 0.31
1041s - loss: 3.1980
Epoch 29/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.14966590881
acc: 0.31
1041s - loss: 3.1982
Epoch 30/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.14809700012
acc: 0.31
1041s - loss: 3.1958
Epoch 31/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.14659920692
acc: 0.305
1041s - loss: 3.1962
Epoch 32/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.14511853218
acc: 0.305
1041s - loss: 3.1958
Epoch 33/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.14368776321
acc: 0.305
1041s - loss: 3.1948
Epoch 34/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.14222851753
acc: 0.305
1041s - loss: 3.1936
Epoch 35/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.14091789246
acc: 0.305
1041s - loss: 3.1940
Epoch 36/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.13962599754
acc: 0.305
1041s - loss: 3.1925
Epoch 37/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.13829167366
acc: 0.305
1041s - loss: 3.1910
Epoch 38/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.13694189072
acc: 0.305
1041s - loss: 3.1884
Epoch 39/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.13562560081
acc: 0.315
1041s - loss: 3.1867
Epoch 40/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.13439843178
acc: 0.315
1041s - loss: 3.1894
Epoch 41/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 3.13324357986
acc: 0.315
