Experiment parameters:
   exp_group = 10_1_soft_wn, exp_id = 1
   Data file (image data): pickle_jar/10_1-1260.p
   nb_epoch = 50, batch_size = 32, model_name = "vgg16"
   Using affinity matrix: data_files/10_1/aff_wordnet_zhao
      soft_label_decay_factor = 3.0
   Validating every 1 epochs.
   Weights are NOT being saved.
   Weights are NOT being loaded.
Loading pickled data...
Done in 2.5 minutes.
Loading affinity matrix for soft labels from text file...
[[ 1.          0.5         0.5         0.14285715  0.33333334  0.14285715
   0.42857143  0.42857143  0.27777779  0.35714287]
 [ 0.5         1.          0.5         0.14285715  0.33333334  0.14285715
   0.42857143  0.42857143  0.27777779  0.35714287]
 [ 0.5         0.5         1.          0.14285715  0.33333334  0.25
   0.54545456  0.54545456  0.27777779  0.45454547]
 [ 0.14285715  0.14285715  0.14285715  1.          0.13333334  0.21428572
   0.14285715  0.14285715  0.11111111  0.14285715]
 [ 0.33333334  0.33333334  0.33333334  0.13333334  1.          0.13333334
   0.33333334  0.33333334  0.72222221  0.46666667]
 [ 0.14285715  0.14285715  0.25        0.21428572  0.13333334  1.          0.2
   0.22222222  0.11111111  0.25      ]
 [ 0.42857143  0.42857143  0.54545456  0.14285715  0.33333334  0.2         1.
   0.69999999  0.27777779  0.45454547]
 [ 0.42857143  0.42857143  0.54545456  0.14285715  0.33333334  0.22222222
   0.69999999  1.          0.27777779  0.45454547]
 [ 0.27777779  0.27777779  0.27777779  0.11111111  0.72222221  0.11111111
   0.27777779  0.27777779  1.          0.3888889 ]
 [ 0.35714287  0.35714287  0.45454547  0.14285715  0.46666667  0.25
   0.45454547  0.45454547  0.3888889   1.        ]]
Re-scaled soft labels.
[[ 1.          0.22313017  0.22313017  0.07642629  0.1353353   0.07642629
   0.18009233  0.18009233  0.11455886  0.1453557 ]
 [ 0.22313017  1.          0.22313017  0.07642629  0.1353353   0.07642629
   0.18009233  0.18009233  0.11455886  0.1453557 ]
 [ 0.22313017  0.22313017  1.          0.07642629  0.1353353   0.10539922
   0.2557292   0.2557292   0.11455886  0.19468674]
 [ 0.07642629  0.07642629  0.07642629  1.          0.07427359  0.09469038
   0.07642629  0.07642629  0.06948344  0.07642629]
 [ 0.1353353   0.1353353   0.1353353   0.07427359  1.          0.07427359
   0.1353353   0.1353353   0.43459818  0.20189653]
 [ 0.07642629  0.07642629  0.10539922  0.09469038  0.07427359  1.
   0.09071794  0.09697195  0.06948344  0.10539922]
 [ 0.18009233  0.18009233  0.2557292   0.07642629  0.1353353   0.09071794
   1.          0.40656966  0.11455886  0.19468674]
 [ 0.18009233  0.18009233  0.2557292   0.07642629  0.1353353   0.09697195
   0.40656966  1.          0.11455886  0.19468674]
 [ 0.11455886  0.11455886  0.11455886  0.06948344  0.43459818  0.06948344
   0.11455886  0.11455886  1.          0.15987976]
 [ 0.1453557   0.1453557   0.19468674  0.07642629  0.20189653  0.10539922
   0.19468674  0.19468674  0.15987976  1.        ]]
Building model...
Building "vgg16" model.
--------------------------------------------------------------------------------
Initial input shape: (None, 3, 224, 224)
--------------------------------------------------------------------------------
Layer (name)                  Output Shape                  Param #             
--------------------------------------------------------------------------------
ZeroPadding2D (zeropadding2d) (None, 3, 226, 226)           0                   
Convolution2D (convolution2d) (None, 64, 224, 224)          1792                
ZeroPadding2D (zeropadding2d) (None, 64, 226, 226)          0                   
Convolution2D (convolution2d) (None, 64, 224, 224)          36928               
MaxPooling2D (maxpooling2d)   (None, 64, 112, 112)          0                   
ZeroPadding2D (zeropadding2d) (None, 64, 114, 114)          0                   
Convolution2D (convolution2d) (None, 128, 112, 112)         73856               
ZeroPadding2D (zeropadding2d) (None, 128, 114, 114)         0                   
Convolution2D (convolution2d) (None, 128, 112, 112)         147584              
MaxPooling2D (maxpooling2d)   (None, 128, 56, 56)           0                   
ZeroPadding2D (zeropadding2d) (None, 128, 58, 58)           0                   
Convolution2D (convolution2d) (None, 256, 56, 56)           295168              
ZeroPadding2D (zeropadding2d) (None, 256, 58, 58)           0                   
Convolution2D (convolution2d) (None, 256, 56, 56)           590080              
ZeroPadding2D (zeropadding2d) (None, 256, 58, 58)           0                   
Convolution2D (convolution2d) (None, 256, 56, 56)           590080              
MaxPooling2D (maxpooling2d)   (None, 256, 28, 28)           0                   
ZeroPadding2D (zeropadding2d) (None, 256, 30, 30)           0                   
Convolution2D (convolution2d) (None, 512, 28, 28)           1180160             
ZeroPadding2D (zeropadding2d) (None, 512, 30, 30)           0                   
Convolution2D (convolution2d) (None, 512, 28, 28)           2359808             
ZeroPadding2D (zeropadding2d) (None, 512, 30, 30)           0                   
Convolution2D (convolution2d) (None, 512, 28, 28)           2359808             
MaxPooling2D (maxpooling2d)   (None, 512, 14, 14)           0                   
ZeroPadding2D (zeropadding2d) (None, 512, 16, 16)           0                   
Convolution2D (convolution2d) (None, 512, 14, 14)           2359808             
ZeroPadding2D (zeropadding2d) (None, 512, 16, 16)           0                   
Convolution2D (convolution2d) (None, 512, 14, 14)           2359808             
ZeroPadding2D (zeropadding2d) (None, 512, 16, 16)           0                   
Convolution2D (convolution2d) (None, 512, 14, 14)           2359808             
MaxPooling2D (maxpooling2d)   (None, 512, 7, 7)             0                   
Flatten (flatten)             (None, 25088)                 0                   
Dense (dense)                 (None, 4096)                  102764544           
Dropout (dropout)             (None, 4096)                  0                   
Dense (dense)                 (None, 4096)                  16781312            
Dropout (dropout)             (None, 4096)                  0                   
Dense (dense)                 (None, 10)                    40970               
--------------------------------------------------------------------------------
Total params: 134301514
--------------------------------------------------------------------------------
Done in 41.06 seconds.
Training model...
Epoch 1/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.30853860855
acc: 0.15
1036s - loss: 5.3282
Epoch 2/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.30334379196
acc: 0.155
1038s - loss: 5.3198
Epoch 3/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.29987211227
acc: 0.18
1037s - loss: 5.3174
Epoch 4/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.29738046646
acc: 0.185
1037s - loss: 5.3149
Epoch 5/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.29552045822
acc: 0.19
1038s - loss: 5.3139
Epoch 6/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.29377954483
acc: 0.2
1038s - loss: 5.3122
Epoch 7/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.29230043411
acc: 0.205
1038s - loss: 5.3095
Epoch 8/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.29093288422
acc: 0.215
1038s - loss: 5.3095
Epoch 9/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.28978633881
acc: 0.22
1038s - loss: 5.3088
Epoch 10/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.28868934631
acc: 0.225
1038s - loss: 5.3085
Epoch 11/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.2876156044
acc: 0.225
1038s - loss: 5.3074
Epoch 12/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.28666347504
acc: 0.225
1038s - loss: 5.3063
Epoch 13/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.2858159256
acc: 0.23
1037s - loss: 5.3060
Epoch 14/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.28504953384
acc: 0.235
1038s - loss: 5.3057
Epoch 15/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.28430812836
acc: 0.235
1038s - loss: 5.3054
Epoch 16/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.28360944748
acc: 0.235
1038s - loss: 5.3048
Epoch 17/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.28294857025
acc: 0.24
1038s - loss: 5.3041
Epoch 18/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.28229581833
acc: 0.24
1038s - loss: 5.3040
Epoch 19/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.28170322418
acc: 0.245
1038s - loss: 5.3042
Epoch 20/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.28114879608
acc: 0.26
1038s - loss: 5.3034
Epoch 21/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.2806145668
acc: 0.265
1038s - loss: 5.3032
Epoch 22/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.2800564003
acc: 0.27
1038s - loss: 5.3025
Epoch 23/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.27954120636
acc: 0.27
1038s - loss: 5.3023
Epoch 24/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.27908054352
acc: 0.27
1038s - loss: 5.3020
Epoch 25/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.2786148262
acc: 0.26
1038s - loss: 5.3016
Epoch 26/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.27820106506
acc: 0.26
1038s - loss: 5.3026
Epoch 27/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.27775934219
acc: 0.26
1038s - loss: 5.3011
Epoch 28/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.27735452652
acc: 0.255
1038s - loss: 5.3013
Epoch 29/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.2769298172
acc: 0.26
1037s - loss: 5.3003
Epoch 30/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.27652793884
acc: 0.26
1038s - loss: 5.3005
Epoch 31/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.27612060547
acc: 0.26
1038s - loss: 5.2989
Epoch 32/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.27573770523
acc: 0.26
1038s - loss: 5.2996
Epoch 33/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.27538141251
acc: 0.26
1038s - loss: 5.2993
Epoch 34/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.27503717422
acc: 0.26
1038s - loss: 5.3010
Epoch 35/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.27471443176
acc: 0.265
1038s - loss: 5.2990
Epoch 36/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.27438369751
acc: 0.27
1038s - loss: 5.2988
Epoch 37/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.27406032562
acc: 0.27
1038s - loss: 5.2985
Epoch 38/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.27373750687
acc: 0.27
1038s - loss: 5.2982
Epoch 39/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.2734258461
acc: 0.27
1038s - loss: 5.2988
Epoch 40/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.27313934326
acc: 0.27
1038s - loss: 5.2986
Epoch 41/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 5.27283578873
acc: 0.27
