Experiment parameters:
   exp_group = 10_1_soft_wn, exp_id = 5
   Data file (image data): pickle_jar/10_1-1260.p
   nb_epoch = 50, batch_size = 32, model_name = "vgg16"
   Using affinity matrix: data_files/10_1/aff_wordnet_zhao
      soft_label_decay_factor = 7.0
   Validating every 1 epochs.
   Weights are NOT being saved.
   Weights are NOT being loaded.
Loading pickled data...
Done in 2.52 minutes.
Loading affinity matrix for soft labels from text file...
[[ 1.          0.5         0.5         0.14285715  0.33333334  0.14285715
   0.42857143  0.42857143  0.27777779  0.35714287]
 [ 0.5         1.          0.5         0.14285715  0.33333334  0.14285715
   0.42857143  0.42857143  0.27777779  0.35714287]
 [ 0.5         0.5         1.          0.14285715  0.33333334  0.25
   0.54545456  0.54545456  0.27777779  0.45454547]
 [ 0.14285715  0.14285715  0.14285715  1.          0.13333334  0.21428572
   0.14285715  0.14285715  0.11111111  0.14285715]
 [ 0.33333334  0.33333334  0.33333334  0.13333334  1.          0.13333334
   0.33333334  0.33333334  0.72222221  0.46666667]
 [ 0.14285715  0.14285715  0.25        0.21428572  0.13333334  1.          0.2
   0.22222222  0.11111111  0.25      ]
 [ 0.42857143  0.42857143  0.54545456  0.14285715  0.33333334  0.2         1.
   0.69999999  0.27777779  0.45454547]
 [ 0.42857143  0.42857143  0.54545456  0.14285715  0.33333334  0.22222222
   0.69999999  1.          0.27777779  0.45454547]
 [ 0.27777779  0.27777779  0.27777779  0.11111111  0.72222221  0.11111111
   0.27777779  0.27777779  1.          0.3888889 ]
 [ 0.35714287  0.35714287  0.45454547  0.14285715  0.46666667  0.25
   0.45454547  0.45454547  0.3888889   1.        ]]
Re-scaled soft labels.
[[ 1.          0.03019738  0.03019738  0.00247875  0.00940356  0.00247875
   0.01831564  0.01831564  0.00637383  0.011109  ]
 [ 0.03019738  1.          0.03019738  0.00247875  0.00940356  0.00247875
   0.01831564  0.01831564  0.00637383  0.011109  ]
 [ 0.03019738  0.03019738  1.          0.00247875  0.00940356  0.00524752
   0.04151012  0.04151012  0.00637383  0.02196771]
 [ 0.00247875  0.00247875  0.00247875  1.          0.00231889  0.00408677
   0.00247875  0.00247875  0.00198483  0.00247875]
 [ 0.00940356  0.00940356  0.00940356  0.00231889  1.          0.00231889
   0.00940356  0.00940356  0.14306667  0.023913  ]
 [ 0.00247875  0.00247875  0.00524752  0.00408677  0.00231889  1.
   0.00369786  0.00432024  0.00198483  0.00524752]
 [ 0.01831564  0.01831564  0.04151012  0.00247875  0.00940356  0.00369786
   1.          0.12245641  0.00637383  0.02196771]
 [ 0.01831564  0.01831564  0.04151012  0.00247875  0.00940356  0.00432024
   0.12245641  1.          0.00637383  0.02196771]
 [ 0.00637383  0.00637383  0.00637383  0.00198483  0.14306667  0.00198483
   0.00637383  0.00637383  1.          0.01387346]
 [ 0.011109    0.011109    0.02196771  0.00247875  0.023913    0.00524752
   0.02196771  0.02196771  0.01387346  1.        ]]
Building model...
Building "vgg16" model.
--------------------------------------------------------------------------------
Initial input shape: (None, 3, 224, 224)
--------------------------------------------------------------------------------
Layer (name)                  Output Shape                  Param #             
--------------------------------------------------------------------------------
ZeroPadding2D (zeropadding2d) (None, 3, 226, 226)           0                   
Convolution2D (convolution2d) (None, 64, 224, 224)          1792                
ZeroPadding2D (zeropadding2d) (None, 64, 226, 226)          0                   
Convolution2D (convolution2d) (None, 64, 224, 224)          36928               
MaxPooling2D (maxpooling2d)   (None, 64, 112, 112)          0                   
ZeroPadding2D (zeropadding2d) (None, 64, 114, 114)          0                   
Convolution2D (convolution2d) (None, 128, 112, 112)         73856               
ZeroPadding2D (zeropadding2d) (None, 128, 114, 114)         0                   
Convolution2D (convolution2d) (None, 128, 112, 112)         147584              
MaxPooling2D (maxpooling2d)   (None, 128, 56, 56)           0                   
ZeroPadding2D (zeropadding2d) (None, 128, 58, 58)           0                   
Convolution2D (convolution2d) (None, 256, 56, 56)           295168              
ZeroPadding2D (zeropadding2d) (None, 256, 58, 58)           0                   
Convolution2D (convolution2d) (None, 256, 56, 56)           590080              
ZeroPadding2D (zeropadding2d) (None, 256, 58, 58)           0                   
Convolution2D (convolution2d) (None, 256, 56, 56)           590080              
MaxPooling2D (maxpooling2d)   (None, 256, 28, 28)           0                   
ZeroPadding2D (zeropadding2d) (None, 256, 30, 30)           0                   
Convolution2D (convolution2d) (None, 512, 28, 28)           1180160             
ZeroPadding2D (zeropadding2d) (None, 512, 30, 30)           0                   
Convolution2D (convolution2d) (None, 512, 28, 28)           2359808             
ZeroPadding2D (zeropadding2d) (None, 512, 30, 30)           0                   
Convolution2D (convolution2d) (None, 512, 28, 28)           2359808             
MaxPooling2D (maxpooling2d)   (None, 512, 14, 14)           0                   
ZeroPadding2D (zeropadding2d) (None, 512, 16, 16)           0                   
Convolution2D (convolution2d) (None, 512, 14, 14)           2359808             
ZeroPadding2D (zeropadding2d) (None, 512, 16, 16)           0                   
Convolution2D (convolution2d) (None, 512, 14, 14)           2359808             
ZeroPadding2D (zeropadding2d) (None, 512, 16, 16)           0                   
Convolution2D (convolution2d) (None, 512, 14, 14)           2359808             
MaxPooling2D (maxpooling2d)   (None, 512, 7, 7)             0                   
Flatten (flatten)             (None, 25088)                 0                   
Dense (dense)                 (None, 4096)                  102764544           
Dropout (dropout)             (None, 4096)                  0                   
Dense (dense)                 (None, 4096)                  16781312            
Dropout (dropout)             (None, 4096)                  0                   
Dense (dense)                 (None, 10)                    40970               
--------------------------------------------------------------------------------
Total params: 134301514
--------------------------------------------------------------------------------
Done in 26.36 seconds.
Training model...
Epoch 1/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.60457760811
acc: 0.27
1041s - loss: 2.6389
Epoch 2/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.58565734863
acc: 0.31
1044s - loss: 2.6183
Epoch 3/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.57162405968
acc: 0.325
1044s - loss: 2.6092
Epoch 4/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.55929410934
acc: 0.325
1044s - loss: 2.5990
Epoch 5/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.5486918354
acc: 0.33
1044s - loss: 2.5935
Epoch 6/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.53889250755
acc: 0.33
1044s - loss: 2.5868
Epoch 7/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.52974171638
acc: 0.325
1044s - loss: 2.5802
Epoch 8/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.52130961418
acc: 0.33
1044s - loss: 2.5748
Epoch 9/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.51348971367
acc: 0.335
1044s - loss: 2.5699
Epoch 10/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.50597324371
acc: 0.335
1044s - loss: 2.5635
Epoch 11/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.49909104347
acc: 0.335
1044s - loss: 2.5613
Epoch 12/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.49260807991
acc: 0.335
1044s - loss: 2.5580
Epoch 13/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.48656730652
acc: 0.33
1044s - loss: 2.5555
Epoch 14/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.48050870895
acc: 0.34
1044s - loss: 2.5478
Epoch 15/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.47503453255
acc: 0.34
1044s - loss: 2.5461
Epoch 16/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.46964477539
acc: 0.34
1044s - loss: 2.5419
Epoch 17/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.46460697174
acc: 0.335
1044s - loss: 2.5397
Epoch 18/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.45968515396
acc: 0.33
1044s - loss: 2.5356
Epoch 19/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.45523831367
acc: 0.335
1044s - loss: 2.5359
Epoch 20/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.45080441475
acc: 0.335
1044s - loss: 2.5311
Epoch 21/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.446357584
acc: 0.335
1043s - loss: 2.5250
Epoch 22/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.44214816093
acc: 0.335
1044s - loss: 2.5253
Epoch 23/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.43836602211
acc: 0.335
1044s - loss: 2.5248
Epoch 24/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.43451947212
acc: 0.335
1044s - loss: 2.5211
Epoch 25/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.4309370327
acc: 0.335
1044s - loss: 2.5188
Epoch 26/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.42760375977
acc: 0.335
1043s - loss: 2.5216
Epoch 27/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.4244713974
acc: 0.335
1044s - loss: 2.5188
Epoch 28/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.42129780769
acc: 0.335
1044s - loss: 2.5128
Epoch 29/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.41822624207
acc: 0.335
1044s - loss: 2.5126
Epoch 30/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.41509782791
acc: 0.34
1044s - loss: 2.5073
Epoch 31/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.41219892502
acc: 0.34
1044s - loss: 2.5107
Epoch 32/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.40940237045
acc: 0.34
1044s - loss: 2.5059
Epoch 33/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.4067940712
acc: 0.34
1044s - loss: 2.5031
Epoch 34/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.40425732613
acc: 0.34
1043s - loss: 2.5058
Epoch 35/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.40158380508
acc: 0.34
1044s - loss: 2.5003
Epoch 36/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.39913228035
acc: 0.34
1044s - loss: 2.5011
Epoch 37/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.39688052177
acc: 0.345
1044s - loss: 2.5007
Epoch 38/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.39452548981
acc: 0.345
1044s - loss: 2.4947
Epoch 39/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.39215974808
acc: 0.34
1043s - loss: 2.4933
Epoch 40/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.3901376915
acc: 0.345
1043s - loss: 2.4970
Epoch 41/50
128/200 [==================>...........] - ETA: 1s200/200 [==============================] - 4s     
val loss: 2.38794560432
acc: 0.355
